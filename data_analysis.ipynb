{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def clean_article(article, remove_stopwords = True):\n",
    "    \"\"\"Helper function to clean the reviews i.e. to convert a document to a sequence of words.\n",
    "     Please note that we're not removing stopwords since word2vec relies on the broader context\n",
    "     of the sentence in order to produce high-quality word vectors.\n",
    "\n",
    "     Arg: review: review string (str)\n",
    "          remove_stopwords: If true remove stopwords else not. (boolean)\n",
    "     Returns: cleaned_review : Cleaned review (list)\n",
    "\n",
    "     You should carry out the following steps.\n",
    "     1. Remove HTML Tags.\n",
    "     2. Remove non-letter characters.\n",
    "     3. Convert to lower case.\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    article_text = BeautifulSoup(article).get_text()     \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", article_text) \n",
    "    words = letters_only.lower().split()    \n",
    "    if remove_stopwords:                         \n",
    "      stops = set(stopwords.words(\"english\"))                  \n",
    "      words = [w for w in words if not w in stops]   \n",
    "    cleaned_article = words\n",
    "\n",
    "    #####################\n",
    "    \n",
    "    return cleaned_article\n",
    "\n",
    "def article_to_sentences( article: str, tokenizer: nltk.tokenize.punkt.PunktSentenceTokenizer ):\n",
    "    \"\"\"Helper function to split a review into parsed sentences. Returns a \n",
    "     list of sentences, where each sentence is a list of words.\n",
    "\n",
    "     Arg: review: review string (str)\n",
    "          tokenizer: punkt tokenizer\n",
    "     Returns:\n",
    "          review_sentences: List of list of tokens.\n",
    "                            e.g. [[\"word2vec\", \"was\", \"introduced\", \"by\", \"google\" ],[\"it\",\"leverages\",\"distributed\",\"token\",\"representations\"]]\n",
    "\n",
    "     You should carry out the following steps.\n",
    "     1. Use the tokenizer to split the paragraph into sentences.\n",
    "     2. Clean the sentence to return a list of words for each sentence using the helper funtion above.\n",
    "     3. Return a list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    sentences = tokenizer.tokenize(article)\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "      cleaned_sentence = clean_article(sentence)\n",
    "      cleaned_sentences.append(cleaned_sentence)\n",
    "    article_sentences = cleaned_sentences\n",
    "    ######################\n",
    "    \n",
    "    return article_sentences\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')\n",
    "train['text'].dropna(inplace=True)\n",
    "test['text'].dropna(inplace=True)\n",
    "\n",
    "sentences = []\n",
    "num_sentences_per_article = []\n",
    "num_tokens_per_sentence = []\n",
    "for article in train['text']:\n",
    "    cleaned_article_sentences = article_to_sentences(article, tokenizer)\n",
    "    num_sentences_per_article.append(len(cleaned_article_sentences))\n",
    "    for sentence in cleaned_article_sentences:\n",
    "        num_tokens_per_sentence.append(len(sentence))\n",
    "        sentences.append(sentence)\n",
    "\n",
    "\n",
    "trained_word2vec_model = word2vec.Word2Vec(sentences, workers=4, \\\n",
    "            size=100, min_count = 40, \\\n",
    "            window = 5, sample = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_articles_train = len(train['text'])\n",
    "num_reliable_train = len(train[(train['label']==0)])\n",
    "num_fake_train = len(train[(train['label']==1)])\n",
    "num_articles_test = len(test['text'])\n",
    "num_words_vocab = len(trained_word2vec_model.wv.vocab)\n",
    "average_sentences_per_article = sum(num_sentences_per_article)/len(num_sentences_per_article)\n",
    "average_tokens_per_sentence = sum(num_tokens_per_sentence)/len(num_tokens_per_sentence)\n",
    "top_10_words = trained_word2vec_model.wv.index2entity[:10]\n",
    "top_10_word_frequencies = [trained_word2vec_model.wv.vocab[word].count for word in top_10_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80054, 66294, 56289, 38670, 37358, 36541, 30061, 27439, 26345, 25448]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(train, w2v_model):\n",
    "    vocab = w2v_model.wv.vocab\n",
    "    distinct_word_count = 0\n",
    "    for i,article in enumerate(train['text']):\n",
    "        print(i)\n",
    "        cleaned_article_sentences = article_to_sentences(article, tokenizer)\n",
    "        words = set([item for sublist in cleaned_article_sentences for item in sublist])\n",
    "        distinct_word_count += len(words)\n",
    "    sparsity = 1 - (distinct_word_count/(len(train)*len(vocab)))\n",
    "    return sparsity\n",
    "\n",
    "sparsity = calculate_sparsity(train,trained_word2vec_model)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate cluster visualization plot\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "def clustering_on_wordvecs(word_vectors, num_clusters=10):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++')\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "    \n",
    "    return kmeans_clustering.cluster_centers_, idx\n",
    "\n",
    "Z = trained_word2vec_model.wv.syn0;\n",
    "centers, clusters = clustering_on_wordvecs(Z, 11);\n",
    "centroid_map = dict(zip(trained_word2vec_model.wv.index2word, clusters));\n",
    "center_words = []\n",
    "for vec in centers:\n",
    "    center_words.append(trained_word2vec_model.most_similar(positive=[vec], topn=1))\n",
    "\n",
    "keys = [word[0][0] for word in center_words]\n",
    "keys.remove('jxhnbinder')\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in trained_word2vec_model.most_similar(word, topn=5):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(trained_word2vec_model[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_similar_words('Simiilarity clusters (n_closest=5) for cluster centers (n_clusters=10) from Kaggle Fake News Dataset', keys, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train = train[(train['label']==1)]\n",
    "real_train = train[(train['label']==0)]\n",
    "fake_train['text'].dropna(inplace=True)\n",
    "real_train['text'].dropna(inplace=True)\n",
    "\n",
    "fake_sentences = []\n",
    "real_sentences = []\n",
    "\n",
    "for article in fake_train['text']:\n",
    "    cleaned_article_sentences = article_to_sentences(article, tokenizer)\n",
    "    for sentence in cleaned_article_sentences:\n",
    "        fake_sentences.append(sentence)\n",
    "        \n",
    "for article in real_train['text']:\n",
    "    cleaned_article_sentences = article_to_sentences(article, tokenizer)\n",
    "    for sentence in cleaned_article_sentences:\n",
    "        real_sentences.append(sentence)\n",
    "        \n",
    "fake_trained_word2vec_model = word2vec.Word2Vec(fake_sentences, workers=4, \\\n",
    "            size=100, min_count = 40, \\\n",
    "            window = 5, sample = 1e-3)\n",
    "real_trained_word2vec_model = word2vec.Word2Vec(real_sentences, workers=4, \\\n",
    "            size=100, min_count = 40, \\\n",
    "            window = 5, sample = 1e-3)\n",
    "\n",
    "top_10_fake_words = fake_trained_word2vec_model.wv.index2entity[:10]\n",
    "top_10_real_words = real_trained_word2vec_model.wv.index2entity[:10]\n",
    "top_10_fake_word_frequencies = [fake_trained_word2vec_model.wv.vocab[word].count for word in top_10_fake_words]\n",
    "top_10_real_word_frequencies = [real_trained_word2vec_model.wv.vocab[word].count for word in top_10_real_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20170, 19293, 16599, 16128, 14941, 14555, 12768, 11875, 10400, 10323]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_fake_word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train['title'].dropna(inplace=True)\n",
    "real_train['title'].dropna(inplace=True)\n",
    "\n",
    "percent_words_capital_fake = []\n",
    "percent_words_capital_real = []\n",
    "for title in fake_train['title']:\n",
    "    num_capital_words = 0\n",
    "    num_words = 0\n",
    "    words = title.split()\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            num_capital_words += 1\n",
    "        num_words += 1\n",
    "    percent_words_capital_fake.append(num_capital_words/num_words)\n",
    "for title in real_train['title']:\n",
    "    num_capital_words = 0\n",
    "    num_words = 0\n",
    "    words = title.split()\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            num_capital_words += 1\n",
    "        num_words += 1\n",
    "    percent_words_capital_real.append(num_capital_words/num_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_percent_capital_fake = sum(percent_words_capital_fake)/len(percent_words_capital_fake)\n",
    "average_percent_capital_real = sum(percent_words_capital_real)/len(percent_words_capital_real)\n",
    "min_percent_capital_fake = min(percent_words_capital_fake)\n",
    "min_percent_capital_real = min(percent_words_capital_real)\n",
    "max_percent_capital_fake = max(percent_words_capital_fake)\n",
    "max_percent_capital_real = max(percent_words_capital_real)\n",
    "\n",
    "def get_percent_capital_boxplots():\n",
    "    # Create a figure instance\n",
    "    plt.title(\"Percent Capital Words for Titles\")\n",
    "    fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "    # Create an axes instance\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xticklabels(['Fake', 'Real'])\n",
    "\n",
    "    # Create the boxplot\n",
    "    bp = ax.boxplot([percent_words_capital_fake,percent_words_capital_real])\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig('fig1.png', bbox_inches='tight')\n",
    "    plt.savefig(\"percent_capital.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "get_percent_capital_boxplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train['title'].dropna(inplace=True)\n",
    "real_train['title'].dropna(inplace=True)\n",
    "\n",
    "title_length_fake = []\n",
    "title_length_real = []\n",
    "for title in fake_train['title']:\n",
    "    words = title.split()\n",
    "    title_length_fake.append(len(words))\n",
    "for title in real_train['title']:\n",
    "    words = title.split()\n",
    "    title_length_real.append(len(words))\n",
    "\n",
    "def get_title_length_boxplots():\n",
    "    # Create a figure instance\n",
    "    plt.title(\"Number of Words for Titles\")\n",
    "    fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "    # Create an axes instance\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xticklabels(['Fake', 'Real'])\n",
    "\n",
    "    # Create the boxplot\n",
    "    bp = ax.boxplot([title_length_fake,title_length_real])\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\"title_length.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "get_title_length_boxplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
